{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainer.optimizer import WeightDecay\n",
    "from chainer import serializers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.training import triggers\n",
    "\n",
    "from chainercv.datasets import voc_bbox_label_names\n",
    "from chainercv.datasets import VOCBboxDataset\n",
    "from chainercv.extensions import DetectionVOCEvaluator\n",
    "from chainercv.links.model.ssd import GradientScaling\n",
    "from chainercv.links.model.ssd import multibox_loss\n",
    "from chainercv.links import SSD300\n",
    "from chainercv.links import SSD512\n",
    "from chainercv import transforms\n",
    "\n",
    "from chainercv.links.model.ssd import random_crop_with_bbox_constraints\n",
    "from chainercv.links.model.ssd import random_distort\n",
    "from chainercv.links.model.ssd import resize_with_random_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatedDataset(chainer.dataset.DatasetMixin):\n",
    "\n",
    "    def __init__(self, *datasets):\n",
    "        self._datasets = datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(dataset) for dataset in self._datasets)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        if i < 0:\n",
    "            raise IndexError\n",
    "        for dataset in self._datasets:\n",
    "            if i < len(dataset):\n",
    "                return dataset[i]\n",
    "            i -= len(dataset)\n",
    "        raise IndexError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiboxTrainChain(chainer.Chain):\n",
    "\n",
    "    def __init__(self, model, alpha=1, k=3):\n",
    "        super(MultiboxTrainChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, imgs, gt_mb_locs, gt_mb_labels):\n",
    "        mb_locs, mb_confs = self.model(imgs)\n",
    "        loc_loss, conf_loss = multibox_loss(\n",
    "            mb_locs, mb_confs, gt_mb_locs, gt_mb_labels, self.k)\n",
    "        loss = loc_loss * self.alpha + conf_loss\n",
    "\n",
    "        chainer.reporter.report(\n",
    "            {'loss': loss, 'loss/loc': loc_loss, 'loss/conf': conf_loss},\n",
    "            self)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(object):\n",
    "\n",
    "    def __init__(self, coder, size, mean):\n",
    "        # to send cpu, make a copy\n",
    "        self.coder = copy.copy(coder)\n",
    "        self.coder.to_cpu()\n",
    "\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, in_data):\n",
    "        # There are five data augmentation steps\n",
    "        # 1. Color augmentation\n",
    "        # 2. Random expansion\n",
    "        # 3. Random cropping\n",
    "        # 4. Resizing with random interpolation\n",
    "        # 5. Random horizontal flipping\n",
    "\n",
    "        img, bbox, label = in_data\n",
    "\n",
    "        # 1. Color augmentation\n",
    "        img = random_distort(img)\n",
    "\n",
    "        # 2. Random expansion\n",
    "        if np.random.randint(2):\n",
    "            img, param = transforms.random_expand(\n",
    "                img, fill=self.mean, return_param=True)\n",
    "            bbox = transforms.translate_bbox(\n",
    "                bbox, y_offset=param['y_offset'], x_offset=param['x_offset'])\n",
    "\n",
    "        # 3. Random cropping\n",
    "        img, param = random_crop_with_bbox_constraints(\n",
    "            img, bbox, return_param=True)\n",
    "        bbox, param = transforms.crop_bbox(\n",
    "            bbox, y_slice=param['y_slice'], x_slice=param['x_slice'],\n",
    "            allow_outside_center=False, return_param=True)\n",
    "        label = label[param['index']]\n",
    "\n",
    "        # 4. Resizing with random interpolatation\n",
    "        _, H, W = img.shape\n",
    "        img = resize_with_random_interpolation(img, (self.size, self.size))\n",
    "        bbox = transforms.resize_bbox(bbox, (H, W), (self.size, self.size))\n",
    "\n",
    "        # 5. Random horizontal flipping\n",
    "        img, params = transforms.random_flip(\n",
    "            img, x_random=True, return_param=True)\n",
    "        bbox = transforms.flip_bbox(\n",
    "            bbox, (self.size, self.size), x_flip=params['x_flip'])\n",
    "\n",
    "        # Preparation for SSD network\n",
    "        img -= self.mean\n",
    "        mb_loc, mb_label = self.coder.encode(bbox, label)\n",
    "\n",
    "        return img, mb_loc, mb_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SSD300(n_fg_class=len(voc_bbox_label_names), pretrained_model='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_preset('evaluate')\n",
    "train_chain = MultiboxTrainChain(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download voc2007 http://host.robots.ox.ac.uk/pascal/VOC/voc2007/#data\n",
    "# Extract content in train and test directory (just take the VOC2007 folder content)\n",
    "data_path = 'data/ssd/voc2007/{}'\n",
    "OUTPUT = 'result'\n",
    "BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TransformDataset(\n",
    "    VOCBboxDataset(\n",
    "        year='2007', split='trainval', data_dir=data_path.format('train')),\n",
    "    Transform(model.coder, model.insize, model.mean))\n",
    "train_iter = chainer.iterators.MultiprocessIterator(train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = VOCBboxDataset(\n",
    "    year='2007', split='test',\n",
    "    use_difficult=True, return_difficult=True, data_dir=data_path.format('test'))\n",
    "test_iter = chainer.iterators.SerialIterator(\n",
    "    test, BATCH_SIZE, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = chainer.optimizers.MomentumSGD()\n",
    "optimizer.setup(train_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to find explaination\n",
    "for param in train_chain.params():\n",
    "    if param.name == 'b':\n",
    "        param.update_rule.add_hook(GradientScaling(2))\n",
    "    else:\n",
    "        param.update_rule.add_hook(WeightDecay(0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (120000, 'iteration'), OUTPUT)\n",
    "trainer.extend(\n",
    "    extensions.ExponentialShift('lr', 0.1, init=1e-3),\n",
    "    trigger=triggers.ManualScheduleTrigger([80000, 100000], 'iteration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.extend(\n",
    "    DetectionVOCEvaluator(\n",
    "        test_iter, model, use_07_metric=True,\n",
    "        label_names=voc_bbox_label_names),\n",
    "    trigger=(10000, 'iteration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10, 'iteration'\n",
    "trainer.extend(extensions.LogReport(trigger=log_interval))\n",
    "trainer.extend(extensions.observe_lr(), trigger=log_interval)\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'iteration', 'lr',\n",
    "     'main/loss', 'main/loss/loc', 'main/loss/conf',\n",
    "     'validation/main/map']),\n",
    ")\n",
    "#     trigger=log_interval)\n",
    "trainer.extend(extensions.ProgressBar(update_interval=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.extend(extensions.snapshot(), trigger=(10000, 'iteration'))\n",
    "trainer.extend(\n",
    "    extensions.snapshot_object(model, 'model_iter_{.updater.iteration}'),\n",
    "    trigger=(120000, 'iteration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
